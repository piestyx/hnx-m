{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HNX-M_v4 Research Notebook\n",
    "## _“Public Research Release”_\n",
    "\n",
    "**A Dual-Strand, Entropy-Gated Neural Architecture for Memory-Efficient Sequence Learning**\n",
    "\n",
    "This notebook documents the architecture, biological inspiration, performance results, and patent claims for the HNX-M_v4 model.\n",
    "\n",
    "> **Note:** This repository contains partial implementation and research results for the patented HNX-M architecture. Core implementation details are proprietary and omitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Modern sequence models such as Transformers, GRUs, and Mamba-based SSMs have improved performance across many domains but still face challenges:\n",
    "- Stability over long sequences\n",
    "- Efficient memory use\n",
    "- Parameter efficiency\n",
    "- Fast convergence\n",
    "\n",
    "HNX-M is a biologically inspired architecture that addresses these issues by combining:\n",
    "- **Dual-strand processing** (forward + backward)\n",
    "- **Shared memory ladder** with learnable rungs\n",
    "- **Entropy-gated modulation** for selective recall\n",
    "- **Asymmetric projection** for strand specialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture Overview\n",
    "\n",
    "### Structural Features – Performance Mapping\n",
    "\n",
    "| Structural Feature                          | Technical Mechanism                                                                 | Primary Impact Area                      | Observed Outcome (Test 10)                                   |\n",
    "|--------------------------------------------|-------------------------------------------------------------------------------------|------------------------------------------|--------------------------------------------------------------|\n",
    "| Dual Strand (Forward & Backward)         | Causal conv1d + reversed scan w/ entropy gating                                     | Bidirectional memory + recurrence        | Faster convergence; handles asymmetric temporal cues         |\n",
    "| Memory Ladder (LearnableDecayMemory)     | Slot-based memory w/ learned decay + Δt bias                                        | Long-term storage & selective recall     | Stable memory access across sequence; no performance dip     |\n",
    "| Gated Memory Injection                   | `sigmoid(mem_gate(fwd_out)) * memory_read`                                         | Precision of memory recall               | Smoother integration of memory; avoids overwriting signal    |\n",
    "| Entropy-Gated Backward Scaling           | Entropy-to-scalar gate modulates backward strand                                   | Backward recall relevance                | Prevents noisy backward signals from derailing output        |\n",
    "| Slot Projection (Asymmetric)             | Separate `slot_proj` and `slot_proj_bwd`                                            | Write/read disentanglement               | Enables divergent memory focus per strand                    |\n",
    "| Per-Slot Write Strengths (NEW)           | `MLP([entropy, delta]) → strength * update`                                        | Write modulation per slot                | Best performance to date; stable loss & highest accuracy     |\n",
    "| Positional Biasing via Learned Δt        | `dt_bias_fwd`, `dt_bias_bwd` injected into positional encoding                     | Temporal pattern sensitivity              | Removes jitter mid-training; helps sustain low final loss    |\n",
    "| Torsion Gate Projection                  | `project_with_torsion_split` with gating via `sigmoid(gate)`                       | Signal sharpening at input stage         | Improves convergence curve shape; reduces early noise        |\n",
    "| Fusion Weights (Learned)                 | Learnable `[fwd, bwd, mem]` vector → weighted sum before output                     | Adaptive decoding                        | Balances pathways dynamically across sequence                |\n",
    "| Rung Signal (Emergent)                   | `compute_state_delta` → `compute_rung_signal`                                       | Memory write trigger granularity         | Still used in logging; not yet re-introduced as gate trigger  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biological Analogies\n",
    "\n",
    "| Structural Feature                          | Biological Analogy                          | Interpretation / Role in System                                                    |\n",
    "|--------------------------------------------|---------------------------------------------|-------------------------------------------------------------------------------------|\n",
    "| Dual Strand (Forward & Backward)         | DNA double helix / bidirectional axons      | Dual propagation pathways for sensing + integration (afferent/efferent signals)    |\n",
    "| Memory Ladder (LearnableDecayMemory)     | Synaptic weight traces / long-term memory   | Selectively reinforced memories with decay → synaptic plasticity                   |\n",
    "| Gated Memory Injection                   | Neurotransmitter gating at dendrites        | Controls which memory traces influence output — like receptor/channel modulation   |\n",
    "| Entropy-Gated Backward Scaling           | Attention focus during cognitive load       | Filters irrelevant memories in high-uncertainty states — like cognitive fatigue    |\n",
    "| Slot Projection (Asymmetric)             | Functional lateralization (brain hemispheres) | Read vs write asymmetry enables diversified representation                         |\n",
    "| Per-Slot Write Strengths                 | Hebbian learning with localized adaptation  | Each \"neuron\" adapts differently to learning signals — slot-level plasticity        |\n",
    "| Positional Biasing via Learned Δt        | Circadian / neural oscillation entrainment  | Aligns processing to internal temporal rhythms — like entrained signal windows     |\n",
    "| Torsion Gate Projection                  | Protein folding via torsional control       | Reshapes signal pathways at input to guide folding/activation                      |\n",
    "| Fusion Weights (Learned)                 | Hormonal modulation / region weight tuning  | Dynamically shifts emphasis between memory, current input, and retrospection       |\n",
    "| Rung Signal (Emergent)                   | Calcium spike / dendritic activation burst  | Triggers when change is high — proxy for firing threshold being crossed            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Code Walkthrough — HNX-M\n",
    "\n",
    "![Architecture Diagram](../figures/architecture_diagram.png)\n",
    "\n",
    "> Diagram illustrating dual strands, shared memory ladder, and gating components.\n",
    "\n",
    "### Code cell 1 - Imports & Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "HNX-M_v4 (Public Research Release)\n",
    "----------------------------------\n",
    "Core `core/` module implementations omitted — covered under patent protection.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from models.metadata import init_meta_logs, update_meta_logs, finalize_meta_logs\n",
    "from core.gates import EntropyGate, compute_entropy\n",
    "from core.scan import StrandScanContrastProcessor\n",
    "from core.rung_logic import compute_state_delta, compute_rung_signal\n",
    "from core.strand import (\n",
    "    StrandProcessor, LearnableDecayMemory,\n",
    "    BackwardScaler, project_with_torsion_split, flip_time\n",
    ")\n",
    "\n",
    "class HNX_M_V4(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_memory_slots):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_proj = nn.Linear(input_dim, 2 * hidden_dim)\n",
    "        self.fusion_weight = nn.Parameter(torch.ones(3))  # [fwd, bwd, mem]\n",
    "        \n",
    "        # Forward & backward strands\n",
    "        self.forward_strand = StrandProcessor(dim=hidden_dim, direction=\"forward\", kernel_size=2)\n",
    "        self.backward_strand = StrandScanContrastProcessor(dim=hidden_dim)\n",
    "        \n",
    "        # Memory ladder\n",
    "        self.memory = LearnableDecayMemory(num_memory_slots, hidden_dim)\n",
    "        self.mem_gate = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Gating & projection\n",
    "        self.entropy_gate = EntropyGate(hidden_dim)\n",
    "        self.backward_scaler = BackwardScaler()\n",
    "        self.slot_proj = nn.Linear(hidden_dim, num_memory_slots)\n",
    "        self.slot_proj_bwd = nn.Linear(hidden_dim, num_memory_slots)\n",
    "        self.output_proj = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Positional biasing\n",
    "        self.dt_bias_fwd = nn.Parameter(torch.zeros(self.hidden_dim))\n",
    "        self.dt_bias_bwd = nn.Parameter(torch.zeros(self.hidden_dim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e2979d",
   "metadata": {},
   "source": [
    "**Structural Features:**\n",
    "- Dual Strand (Forward & Backward)\n",
    "- Memory Ladder\n",
    "- Fusion Weights (Learned)\n",
    "- Positional Biasing via Δt\n",
    "\n",
    "**Biological Analogies:**\n",
    "- DNA double helix / bidirectional axons\n",
    "- Synaptic weight traces\n",
    "- Hormonal modulation / pathway weighting\n",
    "- Circadian / neural oscillation entrainment\n",
    "\n",
    "### Code cell 2 - Forward Method (Part 1: Input & Forward Strand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    meta_logs = init_meta_logs()\n",
    "    B, T, _ = x.shape\n",
    "\n",
    "    # === Torsion Phase + Gate Split ===\n",
    "    x = project_with_torsion_split(x, self.input_proj, self.hidden_dim, self.dt_bias_fwd)\n",
    "\n",
    "    # === Forward Strand Encoding ===\n",
    "    fwd_out = self.forward_strand(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fac96db",
   "metadata": {},
   "source": [
    "**Structural Features:**\n",
    "- Torsion Gate Projection — reshapes signal pathways at input.\n",
    "- Dual Strand (Forward) — processes sequence in natural order.\n",
    "\n",
    "**Biological Analogies:**\n",
    "- Protein folding via torsional control.\n",
    "- Afferent signal pathway in neural systems.\n",
    "\n",
    "\n",
    "### Code cell 3 - Memory Rung Signal & Write Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e8dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Compute Rung Signal ===\n",
    "h_prev = torch.roll(fwd_out, shifts=1, dims=1)\n",
    "delta = compute_state_delta(fwd_out, h_prev)\n",
    "rung_signal = compute_rung_signal(delta)\n",
    "\n",
    "# === Forward Slot Weights + Write ===\n",
    "slot_logits = self.slot_proj(fwd_out)\n",
    "slot_weights = torch.softmax(slot_logits, dim=-1)\n",
    "\n",
    "# === Backward Slot Weights + Read ===\n",
    "slot_weights_bwd = torch.softmax(self.slot_proj_bwd(fwd_out.detach()), dim=-1)\n",
    "mem_out = self.memory.read_all(slot_weights_bwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9ce2d",
   "metadata": {},
   "source": [
    "**Structural Features:**\n",
    "- Rung Signal (Emergent) — detects high state change to trigger memory updates.\n",
    "- Slot Projection (Asymmetric) — separate forward/backward slot projections.\n",
    "- Memory Ladder — selective read/write via attention over rungs.\n",
    "\n",
    "**Biological Analogies:**\n",
    "- Calcium spike / dendritic activation bursts.\n",
    "- Functional lateralisation (brain hemispheres).\n",
    "- Synaptic plasticity.\n",
    "\n",
    "\n",
    "### Code cell 4 - Gating & Backward Strand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e06dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Apply per-token memory gate ===\n",
    "mem_gate = torch.sigmoid(self.mem_gate(fwd_out))\n",
    "mem_out = mem_out * mem_gate\n",
    "\n",
    "# === Backward Strand ===\n",
    "entropy = compute_entropy(fwd_out)\n",
    "entropy_val = entropy.mean(dim=-1)\n",
    "gate_val = self.backward_scaler(entropy_val)\n",
    "\n",
    "bwd_in = flip_time(fwd_out.detach())\n",
    "bwd_scanned = self.backward_strand(bwd_in)\n",
    "bwd_out = flip_time(bwd_scanned)\n",
    "bwd_out = gate_val.unsqueeze(-1) * bwd_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b79ecb6",
   "metadata": {},
   "source": [
    "**Structural Features:**\n",
    "- Gated Memory Injection — precision control of recall.\n",
    "- Entropy-Gated Backward Scaling — filters backward recall when uncertainty is high.\n",
    "\n",
    "**Biological Analogies:**\n",
    "- Neurotransmitter gating at dendrites.\n",
    "- Cognitive load filtering (focus under uncertainty).\n",
    "\n",
    "\n",
    "### Code cell 5 - Output Projection & Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6d17a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Output Projection ===\n",
    "combined = torch.stack([fwd_out, bwd_out, mem_out], dim=0)\n",
    "weighted = (self.fusion_weight[:, None, None, None] * combined).sum(dim=0)\n",
    "return self.output_proj(weighted), finalize_meta_logs(meta_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a65cf96",
   "metadata": {},
   "source": [
    "**Structural Features:**\n",
    "- Fusion Weights (Learned) — adaptive pathway balance.\n",
    "- Final output projection for token-wise predictions.\n",
    "\n",
    "**Biological Analogies:**\n",
    "- Hormonal modulation / dynamic emphasis between sensory, memory, and retrospective signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Patent Coverage\n",
    "\n",
    "See the **Claims** and **Description** sections for full patent text.\n",
    "\n",
    "### Claim 1 (Independent)\n",
    "- Dual-strand (forward + backward) sequence processing\n",
    "- Shared memory ladder with gated rungs\n",
    "- Entropy-based modulation for memory access, positional torsion, and dropout\n",
    "- Output projection combining forward, backward, and memory vectors\n",
    "\n",
    "*(Additional dependent claims follow in patent filing)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experimental Setup\n",
    "\n",
    "- **Environment:** GridWorld variants with dynamic hazards\n",
    "- **Episodes:** 4000\n",
    "- **Epochs:** 10\n",
    "- **Hardware:** AMD RDNA3 GPU (ROCm)\n",
    "- **Metrics:** Accuracy, Loss, Stability\n",
    "\n",
    "### Test 10 Summary\n",
    "- Accuracy: 58.35%\n",
    "- Avg Loss: 0.8813\n",
    "- Train Time: 208.8s\n",
    "- Smoothest convergence yet, no regression across epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results\n",
    "\n",
    "![Loss Curve](../figures/loss_curve_test10.png)\n",
    "\n",
    "![Rung Activation Heatmap](../figures/rung_activation_heatmap.png)\n",
    "\n",
    "- **Observation:** Per-slot write strengths + entropy-gated backward scaling yielded the highest stability and accuracy to date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Discussion\n",
    "\n",
    "- Dual-strand architecture improved early convergence speed compared to baselines.\n",
    "- Entropy gating prevented noisy backward signals from destabilising training.\n",
    "- Asymmetric slot projection allowed strand specialisation, improving memory selectivity.\n",
    "- Biological analogies suggest parallels with synaptic plasticity and hemispheric specialisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion & Future Work\n",
    "\n",
    "HNX-M_v4 demonstrates that biologically inspired sequence architectures can achieve both stability and adaptability.\n",
    "\n",
    "Future research directions:\n",
    "- Mutation masking\n",
    "- Temporal homeostasis\n",
    "- Stochastic rewiring\n",
    "- Synapse sparsity control"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
